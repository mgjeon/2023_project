{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "argparse.Namespace"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.__dict__['config'] = '/userhome/jeon_mg/workspace/codes/NF2/config/isee_noaa_12673_initialize.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/userhome/jeon_mg/workspace/codes/NF2/config/isee_noaa_12673_initialize.json'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(args.config) as config:\n",
    "    info = json.load(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_path': '/userhome/jeon_mg/workspace/_data/isee/noaa_12673/20170906_083600_initialize',\n",
       " 'logging': {'wandb_entity': 'mgjeon',\n",
       "  'wandb_project': 'isee',\n",
       "  'wandb_name': 'noaa_12673_20170906_083600_initialize',\n",
       "  'wandb_id': None},\n",
       " 'data': {'type': 'isee',\n",
       "  'data_path': '/mnt/obsdata/isee_nlfff_v1.2/12673/12673_20170906_083600.nc',\n",
       "  'b_norm': 2500,\n",
       "  'spatial_norm': 320,\n",
       "  'batch_size': 10000.0,\n",
       "  'iterations': 10000.0,\n",
       "  'work_directory': '/userhome/jeon_mg/workspace/_data/_tmp/isee/noaa_12673/20170906_083600_initialize',\n",
       "  'num_workers': 8},\n",
       " 'model': {'dim': 256},\n",
       " 'training': {'lambda_b': {'start': 1000.0, 'end': 1, 'iterations': 50000.0},\n",
       "  'lambda_div': 0.1,\n",
       "  'lambda_ff': 0.1,\n",
       "  'lr_params': {'start': 0.0005, 'end': 5e-05, 'decay_iterations': 100000.0},\n",
       "  'validation_interval': 1000.0}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in info.items():\n",
    "    args.__dict__[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(config='/userhome/jeon_mg/workspace/codes/NF2/config/isee_noaa_12673_initialize.json', base_path='/userhome/jeon_mg/workspace/_data/isee/noaa_12673/20170906_083600_initialize', logging={'wandb_entity': 'mgjeon', 'wandb_project': 'isee', 'wandb_name': 'noaa_12673_20170906_083600_initialize', 'wandb_id': None}, data={'type': 'isee', 'data_path': '/mnt/obsdata/isee_nlfff_v1.2/12673/12673_20170906_083600.nc', 'b_norm': 2500, 'spatial_norm': 320, 'batch_size': 10000.0, 'iterations': 10000.0, 'work_directory': '/userhome/jeon_mg/workspace/_data/_tmp/isee/noaa_12673/20170906_083600_initialize', 'num_workers': 8}, model={'dim': 256}, training={'lambda_b': {'start': 1000.0, 'end': 1, 'iterations': 50000.0}, 'lambda_div': 0.1, 'lambda_ff': 0.1, 'lr_params': {'start': 0.0005, 'end': 5e-05, 'decay_iterations': 100000.0}, 'validation_interval': 1000.0})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/userhome/jeon_mg/workspace/_data/isee/noaa_12673/20170906_083600_initialize'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_path = args.base_path\n",
    "base_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "os.makedirs(base_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/userhome/jeon_mg/workspace/_data/isee/noaa_12673/20170906_083600_initialize/extrapolation_result.nf2'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_path = os.path.join(base_path, 'extrapolation_result.nf2')\n",
    "save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'wandb_entity': 'mgjeon',\n",
       " 'wandb_project': 'isee',\n",
       " 'wandb_name': 'noaa_12673_20170906_083600_initialize',\n",
       " 'wandb_id': None}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "wandb_id = args.logging['wandb_id'] if 'wandb_id' in args.logging else None \n",
    "print(wandb_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_model = args.logging['wandb_log_model'] if 'wandb_log_model' in args.logging else False\n",
    "log_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.loggers import WandbLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmgjeon\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.11 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20231002_135304-z8ok8fb6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mgjeon/isee/runs/z8ok8fb6' target=\"_blank\">noaa_12673_20170906_083600_initialize</a></strong> to <a href='https://wandb.ai/mgjeon/isee' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mgjeon/isee' target=\"_blank\">https://wandb.ai/mgjeon/isee</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mgjeon/isee/runs/z8ok8fb6' target=\"_blank\">https://wandb.ai/mgjeon/isee/runs/z8ok8fb6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb_logger = WandbLogger(project=args.logging['wandb_project'], name=args.logging['wandb_name'], offline=False,\n",
    "                           entity=args.logging['wandb_entity'], id=wandb_id, dir=base_path, log_model=log_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'config': '/userhome/jeon_mg/workspace/codes/NF2/config/isee_noaa_12673_initialize.json',\n",
       " 'base_path': '/userhome/jeon_mg/workspace/_data/isee/noaa_12673/20170906_083600_initialize',\n",
       " 'logging': {'wandb_entity': 'mgjeon',\n",
       "  'wandb_project': 'isee',\n",
       "  'wandb_name': 'noaa_12673_20170906_083600_initialize',\n",
       "  'wandb_id': None},\n",
       " 'data': {'type': 'isee',\n",
       "  'data_path': '/mnt/obsdata/isee_nlfff_v1.2/12673/12673_20170906_083600.nc',\n",
       "  'b_norm': 2500,\n",
       "  'spatial_norm': 320,\n",
       "  'batch_size': 10000.0,\n",
       "  'iterations': 10000.0,\n",
       "  'work_directory': '/userhome/jeon_mg/workspace/_data/_tmp/isee/noaa_12673/20170906_083600_initialize',\n",
       "  'num_workers': 8},\n",
       " 'model': {'dim': 256},\n",
       " 'training': {'lambda_b': {'start': 1000.0, 'end': 1, 'iterations': 50000.0},\n",
       "  'lambda_div': 0.1,\n",
       "  'lambda_ff': 0.1,\n",
       "  'lr_params': {'start': 0.0005, 'end': 5e-05, 'decay_iterations': 100000.0},\n",
       "  'validation_interval': 1000.0}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_logger.experiment.config.update(vars(args), allow_val_change=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'work_directory' not in args.data or args.data['work_directory'] is None:\n",
    "    args.data['work_directory'] = base_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/userhome/jeon_mg/workspace/_data/_tmp/isee/noaa_12673/20170906_083600_initialize'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.data['work_directory']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'isee'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.data['type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'isee',\n",
       " 'data_path': '/mnt/obsdata/isee_nlfff_v1.2/12673/12673_20170906_083600.nc',\n",
       " 'b_norm': 2500,\n",
       " 'spatial_norm': 320,\n",
       " 'batch_size': 10000.0,\n",
       " 'iterations': 10000.0,\n",
       " 'work_directory': '/userhome/jeon_mg/workspace/_data/_tmp/isee/noaa_12673/20170906_083600_initialize',\n",
       " 'num_workers': 8}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_function(b_norm, slices=None, *args, **kwargs):\n",
    "    print(b_norm)\n",
    "    print(slices)\n",
    "\n",
    "    print(args)\n",
    "    print(kwargs)\n",
    "\n",
    "    print(kwargs['data_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500\n",
      "None\n",
      "()\n",
      "{'type': 'isee', 'data_path': '/mnt/obsdata/isee_nlfff_v1.2/12673/12673_20170906_083600.nc', 'spatial_norm': 320, 'batch_size': 10000.0, 'iterations': 10000.0, 'work_directory': '/userhome/jeon_mg/workspace/_data/_tmp/isee/noaa_12673/20170906_083600_initialize', 'num_workers': 8}\n",
      "/mnt/obsdata/isee_nlfff_v1.2/12673/12673_20170906_083600.nc\n"
     ]
    }
   ],
   "source": [
    "test_function(**args.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import netCDF4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nlfff:\n",
    "\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename \n",
    "\n",
    "        nc = netCDF4.Dataset(self.filename, 'r')\n",
    "\n",
    "        nc_x = nc.variables['x']\n",
    "        self.x = np.array(nc_x)\n",
    "        nc_y = nc.variables['y']\n",
    "        self.y = np.array(nc_y)\n",
    "        nc_z = nc.variables['z']\n",
    "        self.z = np.array(nc_z)\n",
    "\n",
    "        nc_bx = nc.variables['Bx']\n",
    "        self.bx = np.array(nc_bx).transpose(2, 1, 0)\n",
    "        nc_by = nc.variables['By']\n",
    "        self.by = np.array(nc_by).transpose(2, 1, 0)\n",
    "        nc_bz = nc.variables['Bz']\n",
    "        self.bz = np.array(nc_bz).transpose(2, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_load(data_path, *args, **kwargs):\n",
    "    data = nlfff(data_path)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = test_load(**args.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/obsdata/isee_nlfff_v1.2/12673/12673_20170906_083600.nc'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(513, 257, 257)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.bx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "from pytorch_lightning import LightningDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchesDataset(Dataset):\n",
    "\n",
    "    def __init__(self, batches_file_paths, batch_size):\n",
    "        self.batches_file_paths = batches_file_paths\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return np.ceil(np.load(list(self.batches_file_paths.values())[0], mmap_mode='r').shape[0] / self.batch_size).astype(np.int32)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = {k: np.copy(np.load(bf, mmap_mode='r')[idx * self.batch_size : (idx + 1) * self.batch_size])\n",
    "                for k, bf in self.batches_file_paths.items()}\n",
    "        return data\n",
    "\n",
    "\n",
    "\n",
    "class RandomCoordinateDataset(Dataset):\n",
    "\n",
    "    def __init__(self, cube_shape, spatial_norm, batch_size):\n",
    "        super().__init__()\n",
    "        cube_shape = np.array([[0, cube_shape[0]-1], [0, cube_shape[1]-1], [0, cube_shape[2]-1]])\n",
    "        self.cube_shape = cube_shape\n",
    "        self.spatial_norm = spatial_norm\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return 1 \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        random_coords = torch.FloatTensor(self.batch_size, 3).uniform_()\n",
    "        for i in range(3):\n",
    "            random_coords[:, i] = (random_coords[:, i] * (self.cube_shape[i, 1] - self.cube_shape[i, 0]) + self.cube_shape[i, 0])\n",
    "        random_coords = random_coords / self.spatial_norm\n",
    "        return random_coords\n",
    "\n",
    "\n",
    "\n",
    "class CubeDataset(Dataset):\n",
    "\n",
    "    def __init__(self, cube_shape, spatial_norm, batch_size=1024, strides=1):\n",
    "        coords = np.stack(np.mgrid[:cube_shape[0]:strides, :cube_shape[1]:strides, :cube_shape[2]:strides], -1)\n",
    "        self.coords_shape = coords.shape[:-1]\n",
    "        coords = torch.tensor(coords / spatial_norm, dtype=torch.float32)\n",
    "        coords = coords.reshape((-1, 3))\n",
    "        self.coords = np.split(coords, np.arange(batch_size, len(coords), batch_size))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.coords)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.coords[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlicesDataModule(LightningDataModule):\n",
    "    \n",
    "    def __init__(self, b_slices, \n",
    "                 height, spatial_norm, b_norm, work_directory,\n",
    "                 batch_size={\"boundary\": 1e4, \"random\": 2e4}, \n",
    "                 iterations=1e5, num_workers=None, error_slices=None, \n",
    "                 height_mapping={'z': [0]}, boundary={\"type\": \"open\"},\n",
    "                 validation_strides=1,\n",
    "                 meta_data=None, plot_overview=False, Mm_per_pixel=None, buffer=None,\n",
    "                 **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        # data parameters\n",
    "        self.height = height \n",
    "        self.spatial_norm = spatial_norm\n",
    "        self.b_norm = b_norm \n",
    "        self.height_mapping = height_mapping\n",
    "        self.meta_data = meta_data \n",
    "        self.Mm_per_pixel = Mm_per_pixel\n",
    "\n",
    "        # train parameters\n",
    "        self.iterations = int(iterations)\n",
    "        self.num_workers = num_workers if num_workers is not None else os.cpu_count()\n",
    "\n",
    "        # load dataset\n",
    "        os.makedirs(work_directory, exist_ok=True)\n",
    "\n",
    "        coords = np.stack(np.mgrid[:b_slices.shape[0], :b_slices.shape[1], :b_slices.shape[2]], -1).astype(np.float32)\n",
    "\n",
    "        cube_shape = [*b_slices.shape[:2], height]\n",
    "        self.cube_shape = cube_shape\n",
    "\n",
    "        # flatten data\n",
    "        coords = coords.reshape((-1, 3)).astype(np.float32)\n",
    "        values = b_slices.reshape((-1, 3)).astype(np.float32)\n",
    "\n",
    "        # normalize data\n",
    "        coords = coords / spatial_norm\n",
    "        values = values / b_norm \n",
    "\n",
    "        # shuffle data\n",
    "        r = np.random.permutation(coords.shape[0])\n",
    "        coords = coords[r]\n",
    "        values = values[r]\n",
    "\n",
    "        # store data to disk\n",
    "        coords_npy_path = os.path.join(work_directory, 'coords.npy')\n",
    "        np.save(coords_npy_path, coords)\n",
    "        values_npy_path = os.path.join(work_directory, 'values.npy')\n",
    "        np.save(values_npy_path, values)\n",
    "\n",
    "        batches_file_paths = {\"coords\": coords_npy_path,\n",
    "                              \"values\": values_npy_path}\n",
    "        \n",
    "        boundary_batch_size = int(batch_size['boundary']) if isinstance(batch_size, dict) else int(batch_size)\n",
    "        random_batch_size = int(batch_size['random']) if isinstance(batch_size, dict) else int(batch_size)\n",
    "\n",
    "        # create data loader\n",
    "        self.dataset = BatchesDataset(batches_file_paths, boundary_batch_size)\n",
    "        self.random_dataset = RandomCoordinateDataset(cube_shape, spatial_norm, random_batch_size)\n",
    "        self.cube_dataset = CubeDataset(cube_shape, spatial_norm, batch_size=boundary_batch_size, strides=validation_strides)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        data_loader = DataLoader(self.dataset, batch_size=None, num_workers=self.num_workers, pin_memory=True,\n",
    "                                 sampler=RandomSampler(self.dataset, replacement=True, num_samples=self.iterations))\n",
    "\n",
    "        random_loader = DataLoader(self.random_dataset, batch_size=None, num_workers=self.num_workers, pin_memory=True,\n",
    "                                   sampler=RandomSampler(self.random_dataset, replacement=True, num_samples=self.iterations))\n",
    "        \n",
    "        return {'boundary' : data_loader, 'random' : random_loader}\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        cube_loader = DataLoader(self.cube_dataset, batch_size=None, num_workers=self.num_workers, pin_memory=True,\n",
    "                                 shuffle=False)\n",
    "        boundary_loader = DataLoader(self.dataset, batch_size=None, num_workers=self.num_workers, pin_memory=True,\n",
    "                                     shuffle=False)\n",
    "        return [cube_loader, boundary_loader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ISEEDataModule(SlicesDataModule):\n",
    "    def __init__(self, data_path,\n",
    "                 slices=None, bin=1, use_bz=False,\n",
    "                 components=False, *args, **kwargs):\n",
    "        data = nlfff(data_path)\n",
    "        b_slices = np.stack([data.bx, data.by, data.bz], -1)\n",
    "\n",
    "        height = b_slices.shape[2]\n",
    "        Mm_per_pixel = np.array([np.diff(data.x)[0], np.diff(data.y)[0], np.diff(data.z)[0]])\n",
    "\n",
    "        super().__init__(b_slices, height, Mm_per_pixel=Mm_per_pixel, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.data['type'] == 'isee':\n",
    "    data_module = ISEEDataModule(**args.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from pytorch_lightning import LightningModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sine(nn.Module):\n",
    "    \n",
    "    def __init__(self, w0=1.):\n",
    "        super().__init__()\n",
    "        self.w0 = w0 \n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.sin(self.w0 * x)\n",
    "    \n",
    "\n",
    "    \n",
    "class BModel(nn.Module):\n",
    "\n",
    "    def __init__(self, num_inputs, num_outputs, dim):\n",
    "        super().__init__()\n",
    "        self.d_in = nn.Linear(num_inputs, dim)\n",
    "        lin = [nn.Linear(dim, dim) for _ in range(8)]\n",
    "        self.linear_layers = nn.ModuleList(lin)\n",
    "        self.d_out = nn.Linear(dim, num_outputs)\n",
    "        self.activation = Sine()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.d_in(x))\n",
    "        for l in self.linear_layers:\n",
    "            x = self.activation(l(x))\n",
    "        x = self.d_out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacobian(output, coords):\n",
    "    jac_matrix = [torch.autograd.grad(output[:, i], coords,\n",
    "                                      grad_outputs=torch.ones_like(output[:, i]).to(output),\n",
    "                                      retain_graph=True, create_graph=True, allow_unused=True)[0]\n",
    "                  for i in range(output.shape[1])]\n",
    "    jac_matrix = torch.stack(jac_matrix, dim=1)\n",
    "    return jac_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict\n",
    "\n",
    "\n",
    "class NF2Module(LightningModule):\n",
    "\n",
    "    def __init__(self, validation_settings, dim=256, \n",
    "                 lambda_b={'start': 1e3, 'end':1, 'iterations': 1e5},\n",
    "                 lambda_div=0.1, lambda_ff=0.1, \n",
    "                 lr_params={'start':5e-4, 'end':5e-5, 'decay_iterations':1e5}, \n",
    "                 meta_path=None, **kwargs):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.validation_settings = validation_settings\n",
    "\n",
    "        model = BModel(3, 3, dim)\n",
    "        self.model = model \n",
    "\n",
    "        self.lr_params = lr_params\n",
    "\n",
    "        if meta_path:\n",
    "            state_dict = torch.load(meta_path)['model'].state_dict() \\\n",
    "                if meta_path.endswith('nf2') else torch.load(meta_path)['m']\n",
    "            model.load_state_dict(state_dict)\n",
    "            logging.info(f'Loaded meta state: {meta_path}')\n",
    "\n",
    "        if isinstance(lambda_b, dict):\n",
    "            self.register_buffer('lambda_B', torch.tensor(lambda_b['start'], dtype=torch.float32))\n",
    "            self.register_buffer('lambda_B_gamma', torch.tensor((lambda_b['end'] / lambda_b['start']) ** (1 / lambda_b['iterations']) \\\n",
    "                                                                if lambda_b['iterations'] > 0 else 0, dtype=torch.float32))\n",
    "            self.register_buffer('lambda_B_end', torch.tensor(lambda_b['end'], dtype=torch.float32))\n",
    "        else:\n",
    "            self.register_buffer('lambda_B', torch.tensor(lambda_b, dtype=torch.float32))\n",
    "            self.register_buffer('lambda_B_gamma', torch.tensor(1, dtype=torch.float32))\n",
    "            self.register_buffer('lambda_B_end', torch.tensor(lambda_b, dtype=torch.float32))\n",
    "        \n",
    "        self.register_buffer('lambda_div', torch.tensor(lambda_div, dtype=torch.float32))\n",
    "        self.register_buffer('lambda_ff', torch.tensor(lambda_ff, dtype=torch.float32))\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        parameters = list(self.model.parameters())\n",
    "\n",
    "        if isinstance(self.lr_params, dict):\n",
    "            lr_start = self.lr_params['start']\n",
    "            lr_end = self.lr_params['end']\n",
    "            decay_iterations = self.lr_params['decay_iterations']\n",
    "        else:\n",
    "            lr_start = self.lr_params\n",
    "            lr_end = self.lr_params \n",
    "            decay_iterations = 1\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(parameters, lr=lr_start)\n",
    "        self.scheduler = torch.optim.lr_scheduler.ExponentialLR(self.optimizer, gamma=(lr_end / lr_start) ** (1 / decay_iterations))\n",
    "\n",
    "        return [self.optimizer], [self.scheduler]\n",
    "    \n",
    "    def training_step(self, batch, batch_nb):\n",
    "        boundary_batch = batch['boundary']\n",
    "        boundary_coords = boundary_batch['coords']\n",
    "        boundary_values = boundary_batch['values']\n",
    "\n",
    "        b = self.model(boundary_coords)\n",
    "        b_diff = torch.abs(b - boundary_values)\n",
    "        b_diff = torch.mean(torch.nanmean(b_diff.pow(2), -1))\n",
    "\n",
    "        loss_dict = {'b_diff': b_diff}\n",
    "\n",
    "        loss = b_diff \n",
    "\n",
    "        loss_dict['loss'] = loss \n",
    "\n",
    "        return loss_dict\n",
    "\n",
    "    def on_train_batch_end(self, outputs, batch, batch_idx):\n",
    "        if self.lambda_B > self.lambda_B_end:\n",
    "            self.lambda_B *= self.lambda_B_gamma\n",
    "        \n",
    "        if self.scheduler.get_last_lr()[0] > 5e-5:\n",
    "            self.scheduler.step()\n",
    "\n",
    "        self.log('Learning Rate', self.scheduler.get_last_lr()[0])\n",
    "        self.log('Lambda B', self.lambda_B)\n",
    "\n",
    "        self.log('train/loss', outputs['loss'])\n",
    "        self.log(\"Training Loss\", {k: v.mean() for k, v in outputs.items()})\n",
    "\n",
    "    @torch.enable_grad()\n",
    "    def validation_step(self, batch, batch_nb, dataloader_idx):\n",
    "        if dataloader_idx == 0:\n",
    "            coords = batch \n",
    "            coords.requires_grad = True \n",
    "\n",
    "            b = self.model(coords)\n",
    "\n",
    "            jac_matrix = jacobian(b, coords)\n",
    "            dBx_dx = jac_matrix[:, 0, 0]\n",
    "            dBx_dy = jac_matrix[:, 0, 1]\n",
    "            dBx_dz = jac_matrix[:, 0, 2]\n",
    "            dBy_dx = jac_matrix[:, 1, 0]\n",
    "            dBy_dy = jac_matrix[:, 1, 1]\n",
    "            dBy_dz = jac_matrix[:, 1, 2]\n",
    "            dBz_dx = jac_matrix[:, 2, 0]\n",
    "            dBz_dy = jac_matrix[:, 2, 1]\n",
    "            dBz_dz = jac_matrix[:, 2, 2]\n",
    "\n",
    "            rot_x = dBz_dy - dBy_dz \n",
    "            rot_y = dBx_dz - dBz_dx \n",
    "            rot_z = dBy_dx - dBx_dy \n",
    "\n",
    "            j = torch.stack([rot_x, rot_y, rot_z], -1)\n",
    "            div = torch.abs(dBx_dx + dBy_dy + dBz_dz)\n",
    "            \n",
    "            return {'b': b.detach(), 'j': j.detach(), 'div': div.detach()}\n",
    "        \n",
    "        elif dataloader_idx == 1:\n",
    "            boundary_coords = batch['coords']\n",
    "            boundary_values = batch['values']\n",
    "\n",
    "            b = self.model(boundary_coords)\n",
    "            b_diff = torch.abs(b - boundary_values)\n",
    "            b_diff = torch.mean(torch.nanmean(b_diff.pow(2), -1))\n",
    "\n",
    "            return {'b_diff': b_diff.detach()}\n",
    "    \n",
    "    def validation_epoch_end(self, outputs_list):\n",
    "        if len(outputs_list) == 0:\n",
    "            return \n",
    "        \n",
    "        # data loader 0\n",
    "        outputs = outputs_list[0]\n",
    "\n",
    "        b = torch.cat([o['b'] for o in outputs]) * self.validation_settings['gauss_per_dB']\n",
    "        j = torch.cat([o['j'] for o in outputs]) * self.validation_settings['gauss_per_dB'] / self.validation_settings['Mm_per_ds']\n",
    "        div = torch.cat([o['div'] for o in outputs]) * self.validation_settings['gauss_per_dB'] / self.validation_settings['Mm_per_ds']\n",
    "        \n",
    "        b_norm = b.pow(2).sum(-1).pow(0.5)\n",
    "        j_norm = j.pow(2).sum(-1).pow(0.5)\n",
    "\n",
    "        jxb = torch.cross(j, b, dim=-1)\n",
    "        jxb_norm = jxb.pow(2).sum(-1).pow(0.5)\n",
    "\n",
    "        eps = 1e-7\n",
    "\n",
    "        angle = jxb_norm / (b_norm * j_norm) \n",
    "        sig = torch.asin(torch.clip(angle, -1. + eps, 1. - eps))\n",
    "        sig = torch.abs(torch.rad2deg(sig))\n",
    "        weighted_sig = np.average(sig.cpu().numpy(), weights=j_norm.cpu().numpy())\n",
    "        sigma_J = np.average(angle.cpu().numpy(), weights=j_norm.cpu().numpy())\n",
    "        theta_J = np.rad2deg(np.arcsin(sigma_J))\n",
    "\n",
    "        div_loss = div / (b_norm + eps)\n",
    "        div_loss = div_loss.mean()\n",
    "\n",
    "        ff_loss = jxb_norm / (b_norm + eps)\n",
    "        ff_loss = ff_loss.mean()\n",
    "\n",
    "        # data loader 1\n",
    "        outputs = outputs_list[1]\n",
    "        \n",
    "        b_diff = torch.stack([o['b_diff'] for o in outputs]).mean() * self.validation_settings['gauss_per_dB']\n",
    "\n",
    "        # log\n",
    "        self.log(\"Validation B_diff\", b_diff)\n",
    "        self.log(\"Validation DIV\", div_loss)\n",
    "        self.log(\"Validation FF\", ff_loss)\n",
    "        self.log(\"Validation Sigma\", weighted_sig)\n",
    "        self.log(\"Validation theta_J\", theta_J)\n",
    "\n",
    "        return {'progress_bar': {'b_diff': b_diff, 'div': div_loss, 'ff': ff_loss, 'sigma': weighted_sig, 'theta_J': theta_J},\n",
    "                'log': {'val/b_diff': b_diff, 'val/div': div_loss, 'val/ff': ff_loss, 'val/sigma': weighted_sig, 'val/theta_J': theta_J}}\n",
    "    \n",
    "    def plot_sample(self, b, n_samples=10):\n",
    "        fig, axs = plt.subplots(3, n_samples, figsize=(n_samples * 4, 12))\n",
    "        heights = np.linspace(0, 1, n_samples) ** 2 * (b.shape[2] - 1)\n",
    "        heights = heights.astype(np.int32)\n",
    "\n",
    "        for i in range(3):\n",
    "            for j, h in enumerate(heights):\n",
    "                v_min_max = np.max(np.abs(b[:, :, h, i]))\n",
    "                axs[i, j].imshow(b[:, :, h, i].T, cmap='gray', vmin=-v_min_max, vmax=v_min_max, origin='lower')\n",
    "                axs[i, j].set_axis_off()\n",
    "        \n",
    "        for j, h in enumerate(heights):\n",
    "            axs[0, j].set_title(f'{h:.01f}')\n",
    "        \n",
    "        fig.tight_layout()\n",
    "\n",
    "        wandb.log({\"Slices\": fig})\n",
    "        plt.close('all')\n",
    "\n",
    "    def on_load_checkpoint(self, checkpoint):\n",
    "        super().on_load_checkpoint(checkpoint)\n",
    "        self.lambda_B *= self.lambda_B_gamma ** checkpoint['global_step']\n",
    "        self.lambda_B = max([self.lambda_B, self.lambda_B_end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dim': 256}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lambda_b': {'start': 1000.0, 'end': 1, 'iterations': 50000.0},\n",
       " 'lambda_div': 0.1,\n",
       " 'lambda_ff': 0.1,\n",
       " 'lr_params': {'start': 0.0005, 'end': 5e-05, 'decay_iterations': 100000.0},\n",
       " 'validation_interval': 1000.0}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_settings = {'cube_shape': data_module.cube_dataset.coords_shape,\n",
    "                       'gauss_per_dB': args.data[\"b_norm\"],\n",
    "                       'Mm_per_ds': data_module.Mm_per_pixel * args.data[\"spatial_norm\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cube_shape': (513, 257, 257),\n",
       " 'gauss_per_dB': 2500,\n",
       " 'Mm_per_ds': array([156.70265184, 204.07787328, 204.07787328])}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nf2 = NF2Module(validation_settings, **args.model, **args.training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gpus = torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(save_path, model, data_module, config):\n",
    "    save_state = {\n",
    "        'model': model,\n",
    "        'cube_shape': data_module.cube_shape,\n",
    "        'b_norm': data_module.b_norm,\n",
    "        'spatial_norm': data_module.spatial_norm,\n",
    "        'meta_data': data_module.meta_data,\n",
    "        'config': config,\n",
    "        'Mm_per_pixel': data_module.Mm_per_pixel\n",
    "    }\n",
    "    torch.save(save_state, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': {'type': 'isee',\n",
       "  'data_path': '/mnt/obsdata/isee_nlfff_v1.2/12673/12673_20170906_083600.nc',\n",
       "  'b_norm': 2500,\n",
       "  'spatial_norm': 320,\n",
       "  'batch_size': 10000.0,\n",
       "  'iterations': 10000.0,\n",
       "  'work_directory': '/userhome/jeon_mg/workspace/_data/_tmp/isee/noaa_12673/20170906_083600_initialize',\n",
       "  'num_workers': 8},\n",
       " 'model': {'dim': 256},\n",
       " 'training': {'lambda_b': {'start': 1000.0, 'end': 1, 'iterations': 50000.0},\n",
       "  'lambda_div': 0.1,\n",
       "  'lambda_ff': 0.1,\n",
       "  'lr_params': {'start': 0.0005, 'end': 5e-05, 'decay_iterations': 100000.0},\n",
       "  'validation_interval': 1000.0}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {'data': args.data, 'model': args.model, 'training': args.training}\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import LambdaCallback, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_callback = LambdaCallback(\n",
    "    on_validation_end=lambda *args: save(save_path, nf2.model, data_module, config)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath = base_path,\n",
    "    every_n_train_steps = int(args.training['validation_interval']),\n",
    "    save_last = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(max_epochs=1,\n",
    "                  logger=wandb_logger,\n",
    "                  devices=n_gpus,\n",
    "                  accelerator='gpu',\n",
    "                  strategy='dp',\n",
    "                  num_sanity_val_steps=0,\n",
    "                  val_check_interval=int(args.training['validation_interval']),\n",
    "                  gradient_clip_val=0.1,\n",
    "                  callbacks=[checkpoint_callback, save_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/userhome/jeon_mg/mambaforge/envs/nf2/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory /userhome/jeon_mg/workspace/_data/isee/noaa_12673/20170906_083600_initialize exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "Restoring states from the checkpoint path at /userhome/jeon_mg/workspace/_data/isee/noaa_12673/20170906_083600_initialize/last.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]\n",
      "\n",
      "  | Name  | Type   | Params\n",
      "---------------------------------\n",
      "0 | model | BModel | 528 K \n",
      "---------------------------------\n",
      "528 K     Trainable params\n",
      "0         Non-trainable params\n",
      "528 K     Total params\n",
      "2.113     Total estimated model params size (MB)\n",
      "Restored all states from the checkpoint file at /userhome/jeon_mg/workspace/_data/isee/noaa_12673/20170906_083600_initialize/last.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93260227803c41b0b8e5ab790afefb85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 1000it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/userhome/jeon_mg/mambaforge/envs/nf2/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:142: UserWarning: You're resuming from a checkpoint that ended before the epoch ended. This can cause unreliable results if further training is done. Consider using an end-of-epoch checkpoint or enabling fault-tolerant training: https://pytorch-lightning.readthedocs.io/en/stable/advanced/fault_tolerant_training.html\n",
      "  rank_zero_warn(\n",
      "/userhome/jeon_mg/mambaforge/envs/nf2/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:48: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(nf2, data_module, ckpt_path='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
